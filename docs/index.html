<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Human-Aware Vision and Language Navigation: Developing the HA3D Simulator for Adaptive Responses in Dynamic Environments</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="./w3.css">
</head>

<body>

<!-- Links (sit on top) -->
<div class="w3-top">
  <div class="w3-row w3-padding w3-black">
    <div class="w3-col s2">
      <a href="#paper" class="w3-button w3-block w3-black">Paper</a>
    </div>
    <div class="w3-col s2">
      <a href="#download" class="w3-button w3-block w3-black">Download</a>
    </div>
    <div class="w3-col s2">
      <a href="https://github.com/lpercc/HA3D_simulator" class="w3-button w3-block w3-black">Code</a>
    </div>
    <div class="w3-col s2">
      <a href="#explore" class="w3-button w3-block w3-black">Explore</a>
    </div>
    <div class="w3-col s2">
      <a href="#changelog" class="w3-button w3-block w3-black">Changelog</a>
    </div>
    <div class="w3-col s2">
      <a href="#contact" class="w3-button w3-block w3-black">Contact</a>
    </div>
  </div>
</div>

<br><br><br>

<div class="w3-container" id="paper">
  <div class="w3-content" style="max-width:850px">
    <h2 class="w3-center w3-padding-32"><b>Human-Aware Vision and Language Navigation: Developing the HA3D Simulator for Adaptive Responses in Dynamic Environments</b></h2>
    <center><img src="./HA3D.jpg" style="width: 100%;"></center>
    <h3><b>Abstract</b></h3>
    <p>
    The vision of agents navigating according to instructions is still far from realization, as current embodied agents often rely on three impractical assumptions: Panoramic Action Space (limits directional granularity), Optimal Expert (unrealistic expectations), and Navigation in a Static Environment (ignores dynamic changes) in Vision and Language Navigation (VLN). These assumptions lead to an overestimation of agents' navigation performance in simulators, which compromises their ability to transfer from Simulation to Reality (Sim2Real). To enable and encourage the exploration of Sim2Real in VLN, we relax these assumptions by suggesting that agents should act in a 'human-aware' way, operating in egocentric action space, learning from sub-optimal experts, and interacting with human-involved dynamic environments, termed `Human-Aware VLN (HA-VLN)' task. This approach allows for Sim2Real without the need for training additional models or adaptors. To support this task, we developed the Human-Aware 3D Simulator (HA3D Simulator), which provides an environment that integrates natural human activities with real-world imagery and collects a human motion dataset with 435 human motion models. Based on this simulator, we introduce a benchmark dataset for fine-grained vision and language navigation tasks considering human integration -- Human-Aware Room-to-Room (HA-R2R) dataset, a more human-like, complex instruction dataset with 21,567 instructions and around 4,500 open-vocabulary words. We introduce a baseline agent named VLN-DT, designed specifically to tackle the increased difficulties posed by our relaxed assumptions. Our findings reveal that current state-of-the-art embodied agents experience a significant performance drop in our task, with losses ranging from a minimum of 19.35\% to a maximum of 71.83\%. This suggests that even advanced agents struggle in dynamic environments involving human activities, indicating an overestimation of their Sim2Real capabilities. We also show that VLN-DT can achieve comparable performance trained on 100\% random data and highlight the weak robustness of traditional VLN agents.
    </p>
    <h3 class="w3-left-align"><b>Paper</b></h3>
    Minghan Li, Heng Li, Zhi-Qi Cheng<br>
    <b><a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf">Human-Aware Vision and Language Navigation: Developing the HA3D Simulator for Adaptive Responses in Dynamic Environments</a></b><br>
    XXX<br>
    Paper | <a href="https://arxiv.org/pdf/xxxx.xxxxx.pdf"> arXiv (low res) </a> | Supplemental Material
    <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace;">
@article{xxx,
  title={Human-Aware Vision and Language Navigation: Developing the HA3D Simulator for Adaptive Responses in Dynamic Environments},
  author={Minghan Li, Heng Li, Zhi-Qi Cheng},
  journal={xxx},
  year={2024}
}</pre>
  </div>
</div>

<div class="w3-container" id="download">
  <div class="w3-content" style="max-width:850px">
    <h3 class="w3-left-align"><b>Dataset Download</b></h3>
    <p>Please fill and sign the <a href="https://github.com/lpercc/HA3D_simulator">XXX@gmail.com</a> to request access to the dataset.</p>
  </div>
</div>

<div class="w3-container" id="code">
  <div class="w3-content" style="max-width:850px">
    <h3 class="w3-left-align"><b>Code</b></h3>
    <p>Please check the <a href="https://github.com/lpercc/HA3D_simulator">git repository</a> for a detailed introduction to the dataset and code for several vision tasks.</p>
  </div>
</div>

<div class="w3-container" id="explore">
  <div class="w3-content" style="max-width:850px">
    <h3 class="w3-left-align"><b>Explore the Simulator</b></h3>
      <!-- Video insertion starts here -->
      <video width="100%" controls>
          <source src="simulator_demo3.webm" type="video/webm">
          Your browser does not support the video tag.
      </video>
      <!-- Video insertion ends here -->
  </div>
  
  <div class="w3-content" style="max-width:850px">
    <h3 class="w3-left-align"><b>Explore the dataset</b></h3>
    <p>Use the web interface <a href="https://github.com/lpercc/HA3D_simulator">here</a> to browse the dataset.</p>
  </div>
</div>

<div class="w3-container" id="changelog">
  <div class="w3-content" style="max-width:850px">
    <h3 class="w3-left-align"><b>Changelog</b></h3>
    <ul>
      <li><b>2024-06-03</b>: Create webpage</li>
    </ul>
  </div>
</div>

<div class="w3-container" id="contact">
  <div class="w3-content" style="max-width:850px">
    <h3 class="w3-left-align"><b>Contact</b></h3>
    Please contact us at <a href="XXX@gmail.com">matterport3d@googlegroups.com</a> if you have any questions.
  </div>
</div>

<br><br><br>

</body>
</html>