<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HA-VLN</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/lpercc" target="_blank">Minghan Li</a><sup>1,*</sup>,
                <a href="https://github.com/DylanLIiii" target="_blank">Heng Li</a><sup>1,*</sup>,
                <a href="https://scholar.google.com/citations?user=uB2He2UAAAAJ&hl=en" target="_blank">Zhi-Qi Cheng</a><sup>1,†</sup>,
                <a href="https://github.com/F1y1113" target="_blank">Yifei Dong</a><sup>2</sup>,
              </span>
              <span class="author-block" style="display: block;">
                <a href="https://scholar.google.com/citations?user=ooVdh_kAAAAJ&hl=en" target="_blank">Yuxuan Zhou</a><sup>3</sup>,
                <a href="https://scholar.google.com/citations?user=bjNZqGAAAAAJ&hl=en" target="_blank">Jun-Yan He</a><sup>4</sup>,
                <a href="https://scholar.google.com/citations?user=NSJY12IAAAAJ&hl=en" target="_blank">Qi Dai</a><sup>5</sup>,
                <a href="https://scholar.google.com/citations?user=gjsxBCkAAAAJ&hl=en" target="_blank">Teruko Mitamura</a><sup>1</sup>,
                <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en" target="_blank">Alexander G Hauptmann</a><sup>1</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Carnegie Mellon University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <sup>2</sup>Columbia University
                    </span>
                    <span class="author-block" style="display: block;">
                      <sup>3</sup>University of Mannheim&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <sup>4</sup>Alibaba Group&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <sup>5</sup>Microsoft Research
                    </span>
                    <span class="eql-cntrb">
                      <small><br><sup>*</sup>Indicates Equal Contribution</small>
                      <small><br><sup>†</sup>Indicates Corresponding author</small>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/lpercc/HA3D_simulator" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Panoramas Observation Demos</h2>
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/1pXnuDYAj8r_video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        "hallway:Someone talking on the phone while pacing."
      </h2>
    </div>
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/2azQ1b91cZZ_video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        "rec/game:Someone setting up a table game."
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/agent_human_interaction.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          HA-VLN scenario: The agent navigates while interacting with dynamic human activities, optimizing its route and maintaining safe distances to address the Sim2Real gap and improve real-world applicability.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/simulator.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          HA3D Simulator annotation process, illustrating the integration of the HAPS dataset, human activity annotation, realistic rendering, and agent-environment interaction. The simulator generates dynamic environments by combining human activities with photorealistic 3D scenes, enabling the HA-VLN task.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/sim_grid.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Single-frame in the HA3D simulator showcase viewpoints with human presence in each scene(120-degree FOV), demonstrating the diversity of human activities and environments. Common indoor regions such as bedrooms, hallways, kitchens, balconies, and bathrooms are displayed. Multiple humans can appear in the same region, as seen in the third row, sixth column, and the fifth row, fifth and sixth columns.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/agents.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Model architectures of VLN-CM (left) and VLN-DT (right) agents. Both utilize a cross-modality fusion module to integrate visual and linguistic information for predicting navigation actions.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/introduction.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Overview of the VLN framework assumptions in the HA3D simulator. The simulator introduces an Ergonomic Action Space, Dynamic Environments, and a Sub-Optimal Expert to bridge the gap between simulated and real-world navigation scenarios. The Ergonomic Action Space limits the agent's field of view to 60 degrees, requiring a more realistic navigation strategy compared to the panoramic view used in traditional VLN tasks. Dynamic Environments incorporate time-varying elements, such as human activities, challenging the agent to adapt its navigation strategy to handle video streams that include people. The Sub-Optimal Expert provides navigation guidance that accounts for human factors and dynamic elements, resulting in a more realistic and human-like navigation strategy compared to the optimal expert model that always finds the shortest path without considering these factors.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/human_activity.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The 145 human activity descriptions in the HAPS Dataset, categorized by their respective indoor regions (highlighted in bold red font). Each region includes 5 carefully selected human activity descriptions that best represent the diversity and relevance of activities within that space.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/robot.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Real-world robot used in our experiments. The robot is Unitree GO1-EDU, a quadruped robot equipped with an NVIDIA Jetson TX2 high-performance computing module for handling computational tasks. The robot features an Inertial Measurement Unit (IMU) for measuring acceleration and rotational speed, a Stereo Fisheye Camera for wide-angle perception of its surroundings, and an Ultrasonic Distance Sensor for measuring the distance between the robot and obstacles.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- dataset -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container has-text-centered">
      <!-- Paper video. -->
      <h2 class="title is-3">Dataset</h2>
      <!-- Dataset link -->
      <span class="link-block">
        <a href="https://drive.google.com/drive/folders/1-ox4u8ciI6CvbHPCbW63nzBKrrzT8AZM?usp=drive_link" target="_blank"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fas fa-file-download"></i>
          </span>
          <span>Download Link</span>
        </a>
      </span>

      <h3 class="title is-4">Human Motion Skeletons in HAPS Dataset</h3>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="20%">
            <!-- Your video file here -->
            <source src="static/videos/samples_07_to_13.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="20%">
            <!-- Your video file here -->
            <source src="static/videos/samples_28_to_34.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="20%">\
            <!-- Your video file here -->
            <source src="static/videos/samples_42_to_48.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>

      <h3 class="title is-4">Human Activity Annotation GUI</h3>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/human_activity_anno.webm"
            type="video/webm">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/fine-tune_human_model.webm"
            type="video/webm">
          </video>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- End dataset -->

<!-- simulator -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <!-- Paper video. -->
      <h2 class="title is-3">Explore the Simulator</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="50%">
            <!-- Your video file here -->
            <source src="static/videos/sim1.webm"
            type="video/webm">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="50%">
            <!-- Your video file here -->
            <source src="static/videos/sim2.webm"
            type="video/webm">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="50%">\
            <!-- Your video file here -->
            <source src="static/videos/sim3.webm"
            type="video/webm">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper simulator -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container has-text-centered">
      <!-- Paper video. -->
      <h2 class="title is-3">Video of Real-World Robots</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe 
            src="//player.bilibili.com/player.html?aid=112614042504793&cid=500001581650472&page=1" 
            frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->
<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/HA_VLN_NeurIPS_24.pdf" width="100%" height="550"></iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @article{xxx,
        title={Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions},
        author={Minghan Li, Heng Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G Hauptmann},
        journal={xxx},
        year={2024}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            Thanks to the Visual Perception and Machine Intelligence Laboratory of Southwest Jiaotong University for providing assistance with the robot experiments.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
